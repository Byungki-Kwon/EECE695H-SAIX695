{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PA3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYbcNgp25Fcf"
      },
      "source": [
        "## **Intro**\n",
        "**Installation of Dependencies**\n",
        "\n",
        "Run the below cell to install PyTorch for this notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpuGp59O3gnq"
      },
      "source": [
        "import torch\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIlKbJiZ6AK0"
      },
      "source": [
        "## **Useful Functions**\n",
        "Run the below cell to initialize the provided functions that will be used in this assignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI6llx4pDSwU"
      },
      "source": [
        "from os import path\n",
        "import wheel\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import Function\n",
        "\n",
        "import urllib\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os, sys, math, random, subprocess\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "from IPython.display import clear_output, Image, display, HTML\n",
        "from google.protobuf import text_format\n",
        "from io import StringIO\n",
        "import PIL.Image\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10, 10)        # large images\n",
        "plt.rcParams['image.interpolation'] = 'nearest'  # don't interpolate: show square pixels\n",
        "plt.rcParams['image.cmap'] = 'gray'  # use grayscale output rather than a (potentially misleading) color heatmap\n",
        "\n",
        "def get_n_params(module):\n",
        "  nparam = 0\n",
        "  for name, param in module.named_parameters():\n",
        "    param_count = 1\n",
        "    for size in list(param.size()):\n",
        "      param_count *= size\n",
        "    nparam += param_count\n",
        "  return nparam\n",
        "\n",
        "def get_model_params(model):\n",
        "  nparam = 0\n",
        "  for name, module in model.named_modules():\n",
        "    nparam += get_n_params(module)\n",
        "  return nparam\n",
        "\n",
        "def np_img_from_url(url):\n",
        "  url_response = urllib.urlopen(url)\n",
        "  img_array = np.array(bytearray(url_response.read()), dtype=np.uint8)\n",
        "  img = cv2.imdecode(img_array, -1)\n",
        "  return img\n",
        "\n",
        "def to_numpy_image(tensor_or_variable):\n",
        "  \n",
        "  # If this is already a numpy image, just return it\n",
        "  if type(tensor_or_variable) == np.ndarray:\n",
        "    return tensor_or_variable\n",
        "  \n",
        "  # Make sure this is a tensor and not a variable\n",
        "  if type(tensor_or_variable) == Variable:\n",
        "    tensor = tensor_or_variable.data\n",
        "  else:\n",
        "    tensor = tensor_or_variable\n",
        "  \n",
        "  # Convert to numpy and move to CPU if necessary\n",
        "  np_img = tensor.detach().cpu().numpy()\n",
        "  \n",
        "  # If there is no batch dimension, add one\n",
        "  if len(np_img.shape) == 3:\n",
        "    np_img = np_img[np.newaxis, ...]\n",
        "  \n",
        "  # Convert from BxCxHxW (PyTorch convention) to BxHxWxC (OpenCV/numpy convention)\n",
        "  np_img = np_img.transpose(0, 2, 3, 1)\n",
        "  \n",
        "  return np_img\n",
        "\n",
        "\n",
        "def to_pytorch_image(np_image):\n",
        "  \n",
        "  # Create a batch dimension\n",
        "  if len(np_image.shape) == 3:\n",
        "    np_image = np_image[np.newaxis, ...]\n",
        "  \n",
        "  # Convert from BxHxWxC (OpenCV/numpy) to BxCxHxW (PyTorch)\n",
        "  np_image = np_image.transpose(0, 3, 1, 2)\n",
        "  \n",
        "  pytorch_img = torch.from_numpy(np_image).float()\n",
        "  \n",
        "  return pytorch_img\n",
        "  \n",
        "def draw_border(image_np, color):\n",
        "  color = np.asarray(color)\n",
        "  s = image_np.shape\n",
        "  image_np = image_np.copy()\n",
        "  image_np[0:5, :, :] = color[np.newaxis, np.newaxis, :]\n",
        "  image_np[:, 0:5, :] = color[np.newaxis, np.newaxis, :]\n",
        "  image_np[s[0]-5:s[0], :, :] = color[np.newaxis, np.newaxis, :]\n",
        "  image_np[:, s[0]-5:s[0], :] = color[np.newaxis, np.newaxis, :]\n",
        "  return image_np\n",
        "\n",
        "def normalize_zero_one_range(tensor_like):\n",
        "  x = tensor_like - tensor_like.min()\n",
        "  x = x / (x.max() + 1e-9)\n",
        "  return x\n",
        "\n",
        "\n",
        "def prep_for_showing(image):\n",
        "  np_img = to_numpy_image(image)\n",
        "  if len(np_img.shape) > 3:\n",
        "    np_img = np_img[0]\n",
        "  np_img = normalize_zero_one_range(np_img)\n",
        "  return np_img\n",
        "\n",
        "  \n",
        "def show_image(tensor_var_or_np, title=None, bordercolor=None):\n",
        "  np_img = prep_for_showing(tensor_var_or_np)\n",
        "  \n",
        "  if bordercolor is not None:\n",
        "    np_img = draw_border(np_img, bordercolor)\n",
        "  \n",
        "  # plot it\n",
        "  np_img = np_img.squeeze()\n",
        "  plt.figure(figsize=(4,4))\n",
        "  plt.imshow(np_img)\n",
        "  plt.axis('off')\n",
        "  if title: plt.title(title)\n",
        "  plt.show()\n",
        "    \n",
        "def show_images(images, correct_list=None, size=128, titles=None):\n",
        "  for i, image in enumerate(images):\n",
        "    bordercolor = ([0,1,0] if correct_list[i] else [1,0,0]) if correct_list else None\n",
        "    show_image(image, bordercolor=bordercolor, title=titles[i] if titles else None)\n",
        "    \n",
        "def show_image_rows(image_lists):\n",
        "  for l in image_lists:\n",
        "    #plt.figure(figsize=(1, len(l)))\n",
        "    #plt.axis('off')\n",
        "    f, axarr = plt.subplots(1,len(l))\n",
        "    #print(axarr)\n",
        "    for i,img in enumerate(l):\n",
        "      img_np = prep_for_showing(img).squeeze()\n",
        "      axarr[i].imshow(img_np)\n",
        "      axarr[i].axis('off')\n",
        "    plt.show()\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "def listFD(url, ext=''):\n",
        "  page = requests.get(url).text\n",
        "  soup = BeautifulSoup(page, 'html.parser')\n",
        "  return [url + '/' + node.get('href') for node in soup.find_all('a') if node.get('href').endswith(ext)]\n",
        "\n",
        "def url_to_image(url):\n",
        "\t# download the image, convert it to a NumPy array, and then read\n",
        "\t# it into OpenCV format\n",
        "  resp = urllib.request.urlopen(url)\n",
        "  image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "  image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "  \n",
        "  # Convert BGR to RGB\n",
        "  image = image[:, :, [2,1,0]]\n",
        "  \n",
        "  return image\n",
        "\n",
        "def url_to_text(url):\n",
        "  resp = urllib.request.urlopen(url)\n",
        "  text = resp.read()\n",
        "  return text.decode(\"utf-8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cuswVGX6KKW"
      },
      "source": [
        "## **Dataset**\n",
        "\n",
        "The following cell implements a Dataset that loads the images that will be used later in this assignment.\n",
        "\n",
        "It's a subclass of torch.utils.data.Dataset, which is a base class for Datasets in pytorch, and it is compatible with torch.utils.data.Dataloader. Dataloder is useful, because it allows us to easily load data in multiple background threads without writing any threading code. This is important, because one of the biggest bottlenecks in neural network training is the rate at which data can be fed into the model.\n",
        "\n",
        "A dataset implements __len__ and __getitem__, which means samples from the dataset can be obtained by indexing it like sample = dataset[5].\n",
        "\n",
        "A dataloader can be iterated and returns batches: batch_0 = iter(dataloader).first()\n",
        "\n",
        "torchvision is useful suite of utilities when doing computer vision work with PyTorch. It already provides implementations for many of the most popular computer vision datasets. In this notebook, we have rolled our own dataset that loads images from a URL.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H6ZVcWZDKt-"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "DATA_URL = \"http://www.cs.cornell.edu/courses/cs5670/2019sp/projects/pa5/assignment_data/dataset/\"\n",
        "IMAGENET_LABELS_URL = \"http://www.cs.cornell.edu/courses/cs5670/2019sp/projects/pa5/assignment_data/imagenet_classes.txt\"\n",
        "DOGS_DIR = \"test-dog\"\n",
        "FOOD_DIR = \"test-food\"\n",
        "\n",
        "IMAGENET_MEAN = torch.FloatTensor([0.485, 0.456, 0.406])\n",
        "IMAGENET_STD = torch.FloatTensor([0.229, 0.224, 0.225])\n",
        "\n",
        "class FoodAndDogDataset(Dataset):\n",
        "  \"\"\"\n",
        "  PyTorch DataLoader compatible dataset that pre-loads images from the dataset\n",
        "  directory on the CS5670 class website. Images are returned as tensors, normalized\n",
        "  with the imagenet normalization.\n",
        "  \"\"\"\n",
        "  def __init__(self, include_classes=\"both\"):\n",
        "    self.dog_paths = listFD(DATA_URL + DOGS_DIR, \".jpg\")\n",
        "    self.food_paths = listFD(DATA_URL + FOOD_DIR, \".jpg\")\n",
        "    \n",
        "    self.dog_labels = [0 for _ in self.dog_paths]\n",
        "    self.food_labels = [1 for _ in self.food_paths]\n",
        "    \n",
        "    self.img_paths = []\n",
        "    self.img_labels = []\n",
        "    \n",
        "    if include_classes in [\"both\", \"dogs\"]:\n",
        "      self.img_paths += self.dog_paths\n",
        "      self.img_labels += self.dog_labels\n",
        "    \n",
        "    if include_classes in [\"both\", \"food\"]:\n",
        "      self.img_paths += self.food_paths\n",
        "      self.img_labels += self.food_labels\n",
        "  \n",
        "    # We might not want to do this on a real dataset, if we have a lot of data.\n",
        "    # In that case we would lazy-download images.\n",
        "    # But for now we will pre-download them, because there's not that many\n",
        "    self.all_images_np = [url_to_image(url).astype(np.float32) / 255 for url in self.img_paths]\n",
        "  \n",
        "    print(f\"Initialized dataset: {include_classes} with {len(self.all_images_np)} images\")\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.img_paths)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    img_i_np = self.all_images_np[idx]\n",
        "    label_i = self.img_labels[idx]\n",
        "\n",
        "    # Convert to pytorch. Copy so that we don't keep re-normalizing the same image multiple times.\n",
        "    img_i_tensor = to_pytorch_image(img_i_np.copy())\n",
        "\n",
        "    # Pre-process the image:\n",
        "    # Normalize using the mean and variance of the ImageNet dataset\n",
        "    # The model was trained from this dataset\n",
        "    img_i_tensor = img_i_tensor - IMAGENET_MEAN[np.newaxis, :, np.newaxis, np.newaxis].expand_as(img_i_tensor)\n",
        "    img_i_tensor = img_i_tensor / IMAGENET_STD[np.newaxis, :, np.newaxis, np.newaxis].expand_as(img_i_tensor)\n",
        "\n",
        "    assert img_i_tensor.size(2) == img_i_tensor.size(3) == 256, \"Images expected to be of shape 1x3x256x256\"\n",
        "    \n",
        "    # Take a center-crop of image, assuming the image is 256x256\n",
        "    img_i_tensor = img_i_tensor[:, :, 16:240, 16:240]\n",
        "\n",
        "    return img_i_tensor[0], label_i\n",
        "    \n",
        "dog_dataset = FoodAndDogDataset(include_classes=\"dogs\")\n",
        "food_dataset = FoodAndDogDataset(include_classes=\"food\")\n",
        "full_dataset = FoodAndDogDataset(include_classes=\"both\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsItdkTj6VkC"
      },
      "source": [
        "## **ILSVRC2012 class labels**\n",
        "In this project, we will be using an AlexNet image-classifier model pre-trained on the ILSVRC2012 dataset. In order to understand the model's output, we need a maping from the output probabilities to the class names. We load this mapping in this cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8hiWNnFEzTc"
      },
      "source": [
        "imagenet_id_to_label = {}\n",
        "labels_file = url_to_text(IMAGENET_LABELS_URL)\n",
        "labels_id2word = labels_file.split(\"\\n\")[:1000]\n",
        "print(labels_id2word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fmD4b_76b71"
      },
      "source": [
        "## **Notes on PyTorch**\n",
        "PyTorch implements many operations on Tensors (such as torch.FloatTensor or torch.LongTensor) in much the same way numpy implements operations on ndarray. Variables (torch.autograd.Variable) wrap tensors and can be used interchangeably. When an operation is performed on a Tensor, the output is a new Tensor. When it is performed on a Variable, the output is also a Variable.\n",
        "\n",
        "All operations involving Variables automatically build a computational graph (provided the requires_grad parameter is set to True on any of the Variable's involved in the operation). This means that at any point, calling backward on any Variable will compute the gradient of that Variable with respect to all other Variables. Thus calling backward on the loss function we're trying to optimize will calculate the gradient w.r.t model parameters, allowing us to implement gradient descent.\n",
        "\n",
        "We can also use .backward to calculate a gradient w.r.t the input, provided the requires_grad attribute on the input variable is set to true. After calling backward, the gradients can be accessed in the .grad attribute of the variable. Here is some fairly typical code that uses this functionality to access gradients with respect to an input variable:\n",
        "\n",
        "Given input and target of type torch.FloatTensor. First we must zero gradients on any variable that already has gradients coputed. On nn.Module, this zeroes gradients of all the model parameters.\n",
        "\n",
        "> some_model.zero_grad()\n",
        "\n",
        "\n",
        "Convert the input into a variable that requires gradient\n",
        "\n",
        "> input_var = Variable(input, requires_grad=True)\n",
        "\n",
        "> target_var = Variable(target)\n",
        "\n",
        "Call the model and calculate the loss\n",
        "\n",
        "> out = some_model(input_var)\n",
        "\n",
        "> loss = some_loss_function(out, target)\n",
        "\n",
        "Run backpropagation\n",
        "\n",
        "> loss.backward()\n",
        "\n",
        "Access gradients of the loss with respect to the input\n",
        "\n",
        "> grad_input = input_var.grad\n",
        "\n",
        "> grad_input_tensor = grad_input.data\n",
        "\n",
        "> grad_input_numpy = grad_input_tensor.cpu().numpy()\n",
        "\n",
        "The underlying Tensor can be accessed as the data attribute given a Variable.\n",
        "\n",
        "PyTorch implements many neural network operations in the torch.nn package. These are usually packaged in a Module, which allows for easy modular design of neural network architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNEnu9hs7F7O"
      },
      "source": [
        "## **AlexNet definition**\n",
        "Below is the Module that implements AlexNet, copied from the torchvision package, with minor modifications for easier use in this assignment. Specifically, because the original implementation uses nn.Sequential containers, the individual layers are not directly accessible. We have made them directly accessible by defining the shortcut_modules attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLbUPbZOF7sE"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "model_urls = {\n",
        "    'alexnet': 'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth',\n",
        "}\n",
        "\n",
        "INPLACE = False\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=1000):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=INPLACE),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=INPLACE),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=INPLACE),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=INPLACE),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=INPLACE),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=INPLACE),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=INPLACE),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "        \n",
        "        self.module_shortcuts = {\n",
        "            \"conv1\": self.features[0],\n",
        "            \"relu1\": self.features[1],\n",
        "            \"conv2\": self.features[3],\n",
        "            \"relu2\": self.features[4],\n",
        "            \"conv3\": self.features[6],\n",
        "            \"relu3\": self.features[7],\n",
        "            \"conv4\": self.features[8],\n",
        "            \"relu4\": self.features[9],\n",
        "            \"conv5\": self.features[10],\n",
        "            \"relu5\": self.features[11],\n",
        "            \n",
        "            \"fc6\": self.classifier[2],\n",
        "            \"fc7\": self.classifier[5],\n",
        "            \"fc8\": self.classifier[6]\n",
        "        }\n",
        "    \n",
        "        \n",
        "    def __getitem__(self, layer_name):\n",
        "      if layer_name in self.module_shortcuts:\n",
        "        return self.module_shortcuts[layer_name]\n",
        "      return None\n",
        "    \n",
        "    \n",
        "    def shortcut_modules(self):\n",
        "      for name, mod in self.module_shortcuts.items():\n",
        "        yield name, mod\n",
        "    \n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), 256 * 6 * 6)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "def alexnet(pretrained=False, **kwargs):\n",
        "    r\"\"\"AlexNet model architecture from the\n",
        "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = AlexNet(**kwargs)\n",
        "    if pretrained:\n",
        "        print(\"Loading pre-trained weights from model zoo\")\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['alexnet']))\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "model = alexnet(pretrained=True, num_classes=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN7JpxbS7Rkw"
      },
      "source": [
        "## **Student TODO Implementations**\n",
        "Implement all of your solutions in this secion\n",
        "\n",
        "**Answer TODO 1 here:**\n",
        "\n",
        "1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjucKbqYGUfk"
      },
      "source": [
        "def convert_ilsvrc2012_probs_to_dog_vs_food_probs(probs_ilsvrc):\n",
        "    \"\"\"\n",
        "    Convert from 1000-class ILSVRC probabilities to 2-class \"dog vs food\"\n",
        "    incices.  Use the variables \"dog_indices\" and \"food_indices\" to map from\n",
        "    ILSVRC2012 classes to our classes.\n",
        "    HINT:\n",
        "    Compute \"probs\" by first estimating the probability of classes 0 and 1,\n",
        "    using probs_ilsvrc.  Stack together the two probabilities along axis 1, and\n",
        "    then normalize (along axis 1).\n",
        "    :param probs_ilsvrc: shape (N, 1000) probabilities across 1000 ILSVRC classes\n",
        "    :return probs: shape (N, 2): probabilities of each of the N items as being\n",
        "        either dog (class 0) or food (class 1).\n",
        "    \"\"\"\n",
        "    # in the ILSVRC2012 dataset, indices 151-268 are dogs and index 924-969 are foods\n",
        "    dog_indices = range(151, 269)\n",
        "    food_indices = range(924, 970)\n",
        "    N, _ = probs_ilsvrc.shape\n",
        "    probs = np.zeros((N, 2)) # placeholder\n",
        "    ############################ TODO 2 BEGIN #################################\n",
        "    find_dog = probs_ilsvrc[:, dog_indices]\n",
        "    find_food = probs_ilsvrc[:, food_indices]\n",
        "\n",
        "    dog_prob_sum = np.sum(find_dog, 1)\n",
        "    food_prob_sum = np.sum(find_food, 1)\n",
        "    prob_sum = dog_prob_sum + food_prob_sum\n",
        "    dog_prob_normalize = dog_prob_sum / prob_sum\n",
        "    food_prob_noramlize = food_prob_sum / prob_sum\n",
        "\n",
        "    probs = np.concatenate([np.expand_dims(dog_prob_normalize, 1), np.expand_dims(food_prob_noramlize, 1)], 1)\n",
        "\n",
        "    ############################ TODO 2 END #################################\n",
        "    return probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjGpqHdbH81b"
      },
      "source": [
        "def get_prediction_descending_order_indices(probs, cidx):\n",
        "    \"\"\"\n",
        "    Returns the ordering of probs that would sort it in descending order\n",
        "    :param probs: (N, 2) probabilities (computed in TODO 2)\n",
        "    :param cidx: class index (0 or 1)\n",
        "    :return list of N indices that sorts the array in descending order\n",
        "    \"\"\"\n",
        "    ############################ TODO 3 BEGIN #################################\n",
        "    temp = probs[:, cidx]\n",
        "    t_order = np.empty_like(temp, dtype=np.uint8)\n",
        "    temp_prob2 = temp \n",
        "    i=0\n",
        "    while i<temp.shape[0]:\n",
        "      max_values = temp_prob2.max()\n",
        "      index = np.where(temp==max_values)\n",
        "      if index[0].shape != 1:\n",
        "        for k in range(index[0].shape[0]):\n",
        "          t_order[i] = index[0][k]\n",
        "          i = i+1\n",
        "      else:\n",
        "        t_order[i] = index[0]\n",
        "        i = i+1\n",
        "      temp_prob2 = np.delete(temp_prob2, np.where(temp_prob2==max_values))\n",
        "    ############################ TODO 3 END #################################\n",
        "    return t_order"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYVFJQ4hIFij"
      },
      "source": [
        "def compute_dscore_dimage(scores, image, class_idx):\n",
        "    \"\"\"\n",
        "    Returns the gradient of s_y (the score at index class_idx) with respect to\n",
        "    the image (data), ds_y / dI.  Note that this is the unnormalized class\n",
        "    score \"s\", not the probability \"p\".\n",
        "    :param scores: (Variable) shape (1000) the output scores from AlexNet for image\n",
        "    :param image: (Variable) shape (1, 3, 224, 244) the input image\n",
        "    :param class_idx: class index in range [0, 999] indicating which class to compute saliency for\n",
        "    :return grad: (Tensor) shape (3, 224, 224), gradient ds_y / dI\n",
        "    \"\"\"\n",
        "    grad = torch.zeros_like(image) # placeholder\n",
        "    ############################ TODO 4 BEGIN #################################\n",
        "    scores[class_idx].backward()\n",
        "    grad = image.grad\n",
        "    gradients_as_arr = np.array(grad.detach())\n",
        "    print(grad.shape)\n",
        "    ############################ TODO 4 END #################################\n",
        "    assert tuple(grad.shape) == (1, 3, 224, 224) # expected shape\n",
        "    return grad[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJOPtZBiraqN"
      },
      "source": [
        "def normalized_sgd_with_momentum_update(image, grad, velocity, momentum, learning_rate):\n",
        "    \"\"\"\n",
        "    :param image: (Variable) shape (1, 3, 224, 244) the current solution\n",
        "    :param grad: (Variable) gradient of the loss with respect to the image\n",
        "    :param velocity: (Variable) momentum vector \"V\"\n",
        "    :param momentum: (float) momentum parameter \"mu\"\n",
        "    :param learning_rate: (float) learning rate \"alpha\"\n",
        "    :return: (Variable) the updated image and momentum vector (image, velocity)\n",
        "    \"\"\"\n",
        "    ############################ TODO 5a BEGIN #################################\n",
        "    constant = np.linalg.norm(grad.detach().cpu())\n",
        "    constant = torch.tensor(constant)\n",
        "    # velocity = momentum*velocity - learning_rate*(grad/torch.linalg.norm(grad))\n",
        "    velocity = momentum*velocity - learning_rate*(grad/constant)\n",
        "\n",
        "    image = image + velocity\n",
        "\n",
        "    \n",
        "    ############################ TODO 5a BEGIN #################################\n",
        "    return image, velocity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wA-6DnYtIH5B"
      },
      "source": [
        "def class_visualization_gradient(target_score, image, target_class, reg_lambda):\n",
        "    \"\"\"\n",
        "    Compute the gradient for make_class_visualization (dL / dI).\n",
        "    :param target_score: (Variable) holding the current score assigned to the target class\n",
        "    :param image: (Variable) shape (1, 3, 224, 224) the current solution\n",
        "    :param target_class: (int) ILSVRC class in range [0, 999]\n",
        "    :param regularization: (float) weight (lambda) applied to the regularizer.\n",
        "    :return grad: (Variable) gradient dL / dI\n",
        "    \"\"\"\n",
        "    grad = torch.zeros_like(image) # placeholder\n",
        "\n",
        "    ############################ TODO 6 BEGIN #################################\n",
        "    target_score[target_class].backward()\n",
        "    grad = image.grad\n",
        "    ############################ TODO 6 END #################################\n",
        "    assert tuple(grad.shape) == (1, 3, 224, 224) # expected shape\n",
        "    return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B85hA8VXIJks"
      },
      "source": [
        "def fooling_image_gradient(target_score, orig_data, image_in, target_class, reg_lambda):\n",
        "    \"\"\"\n",
        "    Compute the gradient for make_fooling_image (dL / dI).\n",
        "    :param target_score: (Variable) holding the current score assigned to the target class\n",
        "    :param orig_data: (Variable) shape (1, 3, 224, 224) holding the original image\n",
        "    :param image_in: (Variable) shape (1, 3, 224, 224) hoding the current solution\n",
        "    :param target_class: (int) ILSVRC class in range [0, 999]\n",
        "    :param reg_lambda: (float) weight applied to the regularizer.\n",
        "    :return grad: (Variable) gradient dL / dI\n",
        "    \"\"\"\n",
        "    grad = torch.zeros_like(image_in) # placeholder\n",
        "    ############################ TODO 5b BEGIN #################################\n",
        "    target_score.backward()\n",
        "    temp_grad = image_in.grad\n",
        "    grad = -temp_grad + reg_lambda*(image_in - orig_data)\n",
        "    ############################ TODO 5b END #################################\n",
        "    assert tuple(grad.shape) == (1, 3, 224, 224) # expected shape\n",
        "    return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ce7Adq97ed3"
      },
      "source": [
        "\n",
        "## **1. AlexNet: Visualizing Structure**\n",
        "The AlexNet model consists of 5 convolutional layers and three fully connected layers. Each of these layers is encapsulated in an nn.Module class that has one or more trainable parameters.\n",
        "\n",
        "We can look the shapes of each of the model parameters using the below cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI93-8tQILA0"
      },
      "source": [
        "model_num_params = 0\n",
        "\n",
        "# Loop through all modules\n",
        "model_num_params = get_n_params(model)\n",
        "print(f\"# params in AlexNet: {model_num_params}\")\n",
        "\n",
        "# Loop through the select modules that we've named:\n",
        "for simple_name, module in model.shortcut_modules():\n",
        "  if hasattr(module, \"weight\"):\n",
        "    print(f\"Module {simple_name} weights: {module.weight.size()} bias: {module.bias.size()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVJslvAR7jVC"
      },
      "source": [
        "## **Visualizing conv1 filters**\n",
        "Filters in conv1 are unique in that they take RGB images as input. This means that we can visualize them as RGB images. For all other layers, we cannot view them as nice little colored squares because they are much higher dimensional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdYgoghkJgWc"
      },
      "source": [
        "def vis_square(data, title=None):\n",
        "    \"\"\"Take a Tensor of shape (n, K, height, width) or (n, K, height, width)\n",
        "       and visualize each (height, width) thing in a grid of size approx. sqrt(n) by sqrt(n)\"\"\"\n",
        "    \n",
        "    if data.size(1) > 3:\n",
        "      data = data.view(-1, 1, data.size(2), data.size(3))\n",
        "        \n",
        "    data = to_numpy_image(data)\n",
        "        \n",
        "    # normalize data for display\n",
        "    data = (data - data.min()) / (data.max() - data.min())\n",
        "    \n",
        "    # force the number of filters to be square\n",
        "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
        "    padding = (((0, n ** 2 - data.shape[0]),\n",
        "               (0, 2), (0, 2))                 # add some space between filters\n",
        "               + ((0, 0),) * (data.ndim - 3))  # don't pad the last dimension (if there is one)\n",
        "    data = np.pad(data, padding, mode='constant', constant_values=1)  # pad with ones (white)\n",
        "    \n",
        "    # tile the filters into an image\n",
        "    data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3) + tuple(range(4, data.ndim + 1)))\n",
        "    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
        "    \n",
        "    data = data.squeeze()\n",
        "    \n",
        "    # plot it\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(data)\n",
        "    plt.axis('off')\n",
        "    if title: plt.title(title)\n",
        "      \n",
        "\n",
        "weights = model.features[0].weight.data\n",
        "vis_square(weights, title=\"Visualizing filters in conv1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGBDlQh57p_X"
      },
      "source": [
        "## **Visualizing AlexNet activations.**\n",
        "PyTorch uses dynamic computational graphs. As a result of this, there is no way for us to directly access the intermediate activations as there is no fixed placeholder for them. In PyTorch, we can access intermediate activations using hooks. A forward hook is a function that gets called every time that the forward method on a module has been executed. To access intermediate activations, we register a forward hook that displays the activations.\n",
        "\n",
        "Let's take an example image and pass it through AlexNet.\n",
        "\n",
        "Visualize the activations inside all of these layers, using the code below. Lighter values have higher magnitude, and darker values have smaller magnitude."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3JsngIXJk-0"
      },
      "source": [
        "# Load the image of the dog\n",
        "example_image, label = dog_dataset[10]\n",
        "show_image(example_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCF-oq7vJ0Pj"
      },
      "source": [
        "import functools\n",
        "\n",
        "# Re-define the model to clear any previously registered hooks\n",
        "model = alexnet(pretrained=True, num_classes=1000)\n",
        "\n",
        "# Define a hook that visualizes a layer output\n",
        "def show_activations_hook(name, module, input, output):\n",
        "  \n",
        "  print(f\"Visualizing layer: {name}\")\n",
        "  # For conv/relu layer outputs (BxCxHxW) we plot an image as before\n",
        "  if output.dim() == 4:\n",
        "    vis_square(output, f\"Activations on: {name}\")\n",
        "  \n",
        "  # For linear layer outputs, we plot the activations as a line plot\n",
        "  else:\n",
        "    feat = output.data.view([-1]).cpu().numpy()\n",
        "    plt.figure(figsize=(15, 3))\n",
        "    plt.plot(feat)\n",
        "    plt.title(f\"Activations on: {name}\")\n",
        "  \n",
        "# Register the hook on the select set of modules\n",
        "for name, module in model.shortcut_modules():\n",
        "  hook = functools.partial(show_activations_hook, name)\n",
        "  module.register_forward_hook(hook)\n",
        "  \n",
        "# PyTorch modules work on minibatches and expect the first axis to be the batch axis\n",
        "# If we run a model on a single image, we must turn this into a batch of size 1\n",
        "model_input = Variable(example_image[np.newaxis, ...])\n",
        "\n",
        "# Run the forward pass on the model\n",
        "class_activations = model(model_input)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TYutQ9P7xo9"
      },
      "source": [
        "## **?? Question ??**\n",
        "Consider the visualizations produced by the above cell. Why does fc8 have negative values, but fc6 and fc7 are only positive?\n",
        "\n",
        "HINT: Look at the structure of the network above, and our selection of named layers in AlexNet.module_shortcuts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smQlsXrb70Po"
      },
      "source": [
        "## **Looking up the class names**\n",
        "The AlexNet model above outputs unconstrained class \"scores\". To turn these scores into a valid probability distribution over the 1000 ImageNet classes, we apply the softmax activation. We can find the index of the class with the maximum score and map that back to a description of the class in words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnve5XjdKtZl"
      },
      "source": [
        "# Load the image of the hot dog\n",
        "example_image, label = dog_dataset[10]\n",
        "show_image(example_image)\n",
        "\n",
        "# Re-define the model to clear any previously registered hooks\n",
        "model = alexnet(pretrained=True, num_classes=1000)\n",
        "\n",
        "# Set to eval mode to disable dropout\n",
        "model.eval()\n",
        "\n",
        "model_input = example_image[np.newaxis, ...]\n",
        "\n",
        "model_input = model_input.clone()\n",
        "\n",
        "# Run the forward pass on the model\n",
        "class_activations = model(Variable(model_input))[0]\n",
        "\n",
        "# Compute class probabilities\n",
        "class_probs = F.softmax(class_activations, dim=0)\n",
        "\n",
        "# Get the class index\n",
        "prob, class_idx = torch.max(class_probs, 0)\n",
        "\n",
        "# Take the integer index out of the variable and tensor\n",
        "class_idx = class_idx.data.item()\n",
        "\n",
        "predicted_class_name = labels_id2word[class_idx]\n",
        "print(f\"Predicted class: {class_idx} - {predicted_class_name} with probability {prob.data.item()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fkb7MVb73zk"
      },
      "source": [
        "## **What to expect**\n",
        "\n",
        "We can see that this example was correctly classified, and with high confidence, despite the dog wearing a misleading costume!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZz38vfe774S"
      },
      "source": [
        "## **2. Dog vs Food: Classification**\n",
        "Let's classify dog vs food. We have prepared a test set of dogs dressed up like hotdogs, and hotdogs cut to look like animals. The below cell visualizes all the images in our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GYeD_K3MBa0"
      },
      "source": [
        "classes = ['dog', 'food']\n",
        "class_datasets = [dog_dataset, food_dataset]\n",
        "\n",
        "for cidx, cname in enumerate(classes):\n",
        "  for i, (image, label) in enumerate(class_datasets[cidx]):\n",
        "    if i >= 3:\n",
        "      break\n",
        "    show_image(image, classes[label])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9GNzP6s7-lL"
      },
      "source": [
        "## **Repurposing the ILSVRC2012 Classifier**\n",
        "AlexNet was trained to recognize one of 1000 classes. We can repurpose it for our \"food vs dog\" task by remapping the categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3ZAMjOHMZ7j"
      },
      "source": [
        "# Re-define the model to clear any previously registered hooks\n",
        "model = alexnet(pretrained=True, num_classes=1000)\n",
        "N = len(full_dataset)\n",
        "dataloader = DataLoader(\n",
        "    full_dataset,\n",
        "    batch_size=1024,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=False,\n",
        "    drop_last=False)\n",
        "\n",
        "assert len(dataloader) == 1, \"Since batch_size is bigger than the number of examples, we should only have one batch\"\n",
        "\n",
        "for batch in dataloader:\n",
        "  images = batch[0]\n",
        "  labels = batch[1]\n",
        "  labels_np = labels.numpy()\n",
        "  \n",
        "  class_activations = model(Variable(images))\n",
        "  ilsvrc_class_probs = F.softmax(class_activations, dim=1)\n",
        "  \n",
        "  # Convert from Variable containing FloatTensor to numpy ndarray\n",
        "  ilsvrc_class_probs_np = ilsvrc_class_probs.data.cpu().numpy()\n",
        "\n",
        "  dogfood_class_probs_np = convert_ilsvrc2012_probs_to_dog_vs_food_probs(ilsvrc_class_probs_np)\n",
        "\n",
        "  assert list(dogfood_class_probs_np.shape) == [N, 2]\n",
        "  \n",
        "  np.testing.assert_almost_equal(np.sum(dogfood_class_probs_np, axis=1), np.ones(N), decimal=5)\n",
        "  \n",
        "  print(\"Seems correct!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD9cD0B88DUu"
      },
      "source": [
        "## **Measuring the accuracy**\n",
        "You should expect to get ~90% accuracy for dogs and ~96% accuracy for food with an overall accuracy of 93%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGP-pksXSOjc"
      },
      "source": [
        "predicted_class = np.argmax(dogfood_class_probs_np, axis=1)\n",
        "correct_mask = predicted_class == labels_np\n",
        "num_correct = np.sum(correct_mask)\n",
        "accuracy = 100.0 * float(num_correct) / N\n",
        "\n",
        "print(f\"Overall accuracy {accuracy} {num_correct} / {N}\")\n",
        "\n",
        "for cidx, cname in enumerate(classes):\n",
        "  cls_mask = labels_np == cidx\n",
        "  predicted_cls = predicted_class[cls_mask]\n",
        "  num_correct = np.sum(predicted_cls == cidx)\n",
        "  cls_acc = 100.0 * float(num_correct) / cls_mask.sum()\n",
        "\n",
        "  print(f\"{cname} class accuracy {cls_acc} {num_correct} / {cls_mask.sum()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOR15_v18G-U"
      },
      "source": [
        "## **3. Dog vs Food: Visualization**\n",
        "We can sort the predictions by the \"dog\" score and \"food\" score. The images are sorted according to how much AlexNet thinks the image belongs to that category.\n",
        "\n",
        "Images are colored green/red depending on whether the prediction was correct.\n",
        "\n",
        "For this assignment, you should expect that the incorrect predictions are all near the bottom (with the lowest score). This is a property that is very desirable -- mistakes only happen with lower scores. Note that in general real-world tasks, this does not always happen for free. If you want to be able to estimate the confidence of being correct, you need to separately predict that, and it is not easy to predict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BWksyURTPdp"
      },
      "source": [
        "for cidx, cname in enumerate(classes):\n",
        "  print(f\"Predictions for class: {cname}\")\n",
        "  cls_mask = labels_np == cidx\n",
        "  predicted_cls = predicted_class[cls_mask]\n",
        "  cls_probs = dogfood_class_probs_np[cls_mask]\n",
        "  correct = [p == cidx for p in predicted_cls]\n",
        "  \n",
        "  cls_images = [images[i] for i in range(len(images)) if cls_mask[i]]\n",
        "  \n",
        "  order = get_prediction_descending_order_indices(cls_probs, cidx)\n",
        "  \n",
        "  print(order)\n",
        "  assert len(order) == cls_mask.sum()\n",
        "  \n",
        "  show_images(\n",
        "    images = [cls_images[i] for i in order],\n",
        "    correct_list = [correct[i] for i in order],\n",
        "    titles = [f\"Prob: {cls_probs[i, cidx]}\" for i in order],\n",
        "    size=128\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bYxYbxn8LRO"
      },
      "source": [
        "## **4. Visualizing saliency**\n",
        "Using our pre-trained AlexNet, we will compute class saliency maps as described in Section 3.1 of [2]. As mentioned in Section 2 of the paper, you should compute the gradient with respect to the image of the unnormalized class score (fc8), not of the normalized class probability (prob). You will need to use the backward method of the module to compute gradients with respect to the image.\n",
        "\n",
        "We want to compute:$${\\partial s_y \\over \\partial I}$$\n",
        "\n",
        "where $s_y$ is the score for class $y$ after layer fc8, but before applying the Softmax layer.\n",
        "\n",
        "We will then visualize the squared magnitude of this (max across color channels), to estimate the saliency of the class across the input image. See [1] for more details and intuition.\n",
        "\n",
        "NOTE: You don't need to call model() in your function; this has already been run for you. Same for all gradient functions you implement below.\n",
        "\n",
        "[[2] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. \"Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\", ICLR Workshop 2014.](https://arxiv.org/pdf/1312.6034.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_TuOUSAg5O_"
      },
      "source": [
        "model = alexnet(pretrained=True)\n",
        "\n",
        "def visualize_saliency(image):\n",
        "    image_in = Variable(image.unsqueeze(0), requires_grad=True)\n",
        "    print(torch.max(image_in))\n",
        "    cls_scores = model(image_in)[0]\n",
        "    max_score, max_idx = torch.max(cls_scores, 0)\n",
        "\n",
        "    grad = compute_dscore_dimage(cls_scores, image_in, max_idx)\n",
        "    print(grad.shape)\n",
        "    vis = grad * grad\n",
        "    vis, _ = torch.max(vis, 0)\n",
        "    \n",
        "    return vis\n",
        "\n",
        "class_datasets = [dog_dataset, food_dataset]\n",
        "  \n",
        "num_images = 6\n",
        "for cidx, cname in enumerate(classes):\n",
        "  \n",
        "  print(f\"Saliency for class: {cname}\")\n",
        "  in_images = []\n",
        "  vis_images = []\n",
        "  \n",
        "  for i, sample in enumerate(class_datasets[cidx]):\n",
        "    image_i = sample[0]\n",
        "    label_i = sample[1]\n",
        "  \n",
        "    vis_image = visualize_saliency(image_i)\n",
        "    \n",
        "    assert list(vis_image.shape) == [224, 224]\n",
        "    \n",
        "    vis_images.append(vis_image.unsqueeze(0))\n",
        "    in_images.append(image_i)\n",
        "    if i >= num_images:\n",
        "      break\n",
        "  \n",
        "  row_list = list(zip(in_images, vis_images))\n",
        "  \n",
        "  show_image_rows(row_list)\n",
        "  \n",
        "  cls_mask = labels_np == cidx\n",
        "  predicted_cls = predicted_class[cls_mask]\n",
        "  cls_probs = dogfood_class_probs_np[cls_mask]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kOl6BOm8mYD"
      },
      "source": [
        "## **5. Fooling AlexNet**\n",
        "For many machine learning models, it is possible to \"fool\" them by tweaking the image slightly so that the image is predicted to become any category [1]. Given any image, and any target class, you can perform gradient ascent to maximize the score of that target class (equivalently, gradient descent on the negative score of that class), stopping when the network confidently predicts it as the target class.\n",
        "\n",
        "Again, maximize the score with respect to the unnormalized class score (fc8) and not the normalized class probability (prob).\n",
        "\n",
        "In addition to maximizing the score (minimizing the negative score), also add a regularizer that computes the L2 norm between the original image, and the fooling image. The final gradient will be the sum of the gradient from the regularizer and the gradient from maximizing the class score.\n",
        "\n",
        "We can write this as a loss $L$:$$\n",
        "L = -s_y(I) + R(I)\n",
        "$$\n",
        "\n",
        "where $$ R(I) = 0.5 {\\lambda} \\|I - I_\\text{orig}\\|_2^2 $$$y$ is the target class, and $\\lambda$ is the regularization.\n",
        "\n",
        "Momentum\n",
        "\n",
        "When optimizing functions with ConvNets, typically you use use gradient descent with momentum, which has the update rule:$$V_t = \\mu V_{t-1} - \\alpha G$$$$I_t = I_{t-1} + V_{t}$$\n",
        "\n",
        "where $V$ is the velocity, $\\alpha$ is the learning rate, $\\mu$ is the momentum parameter, $t$ is the iteration number, and $G = \\frac{\\partial L}{\\partial I_{t-1}}$ is the gradient.\n",
        "\n",
        "To improve stability, we will use a slightly different update, which normalizes the gradient $G$ to have unit norm:$$V_t = \\mu V_{t-1} - \\alpha \\frac{G}{\\|G\\|}$$$$I_t = I_{t-1} + V_{t}$$\n",
        "\n",
        "The norm ${\\|G\\|}$ is the 2-norm of all the elements in $G$ flattened into a vector.\n",
        "\n",
        "[1] Szegedy et al, \"Intriguing properties of neural networks\", ICLR 2014"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjIBpNXZpWqN"
      },
      "source": [
        "model = alexnet(pretrained=True)\n",
        "\n",
        "# Set the model to eval mode\n",
        "model.eval()\n",
        "\n",
        "def make_fooling_image(image, target_class, learning_rate, regularization,\n",
        "                       num_iter, momentum, threshold=0.9):\n",
        "    \"\"\"\n",
        "    Fool AlexNet into thinking that any image has a particular class, by perturbing it just a little bit\n",
        "    \n",
        "    :param image: starting image CxHxW tensor\n",
        "    :param target_class: the class that this will become after optimization\n",
        "    :param learning_rate: either a constant, or a function that returns the learning rate at each iteration\n",
        "    :param regularization: lambda parameter to multiply the regularizer\n",
        "    :param num_iter: maximum number of iterations\n",
        "    :param momentum: amount of momentum to use in the SGD \n",
        "    :param threshold: the target score for target_class\n",
        "    \"\"\"\n",
        "    # Create batch dimension and turn into a variable\n",
        "    image = image[np.newaxis, ...]\n",
        "    \n",
        "    print(f\"Fooling AlexNet into thinking this is a: {labels_id2word[target_class]}\")\n",
        "    \n",
        "    # This is the original image (used by the regularizer)\n",
        "    orig_data = Variable(image.clone())\n",
        "    \n",
        "    image_in = Variable(image, requires_grad=True)\n",
        "    velocity = torch.zeros_like(image_in)\n",
        "        \n",
        "    for i in range(num_iter):\n",
        "        curr_scores = model(image_in)[0]\n",
        "        curr_probs = F.softmax(curr_scores, 0)\n",
        "        \n",
        "        target_prob = curr_probs[target_class]\n",
        "        target_score = curr_scores[target_class]\n",
        "                \n",
        "        # compute the gradient\n",
        "        grad_wrt_image = fooling_image_gradient(\n",
        "            target_score, orig_data, image_in, target_class, regularization)\n",
        "        \n",
        "        # update the image with the SGD rule\n",
        "        image_in, velocity = normalized_sgd_with_momentum_update(\n",
        "            image_in, grad_wrt_image, velocity, momentum, learning_rate)\n",
        "        \n",
        "        # Detach the image and velocity so that we don't backprop through\n",
        "        # multiple iterations of the loop\n",
        "        image_in = Variable(image_in.data, requires_grad = True)\n",
        "        velocity = Variable(velocity.data, requires_grad = False)\n",
        "        \n",
        "        # Zero the gradients\n",
        "        model.zero_grad()\n",
        "        \n",
        "        # Take the target probability out of the variable (turn it into float)\n",
        "        target_prob = target_prob.data.item()\n",
        "        \n",
        "        # visualize the current state\n",
        "        print(f\"({i+1}/{num_iter}), {target_prob * 100} confidence\")\n",
        "        \n",
        "        if target_prob > threshold:\n",
        "            break\n",
        "    \n",
        "    delta = (image_in - orig_data).data\n",
        "    return image_in, delta\n",
        "\n",
        "\n",
        "num_images = 2\n",
        "target_class=113\n",
        "for cidx, cname in enumerate(classes):\n",
        "    dataset = class_datasets[cidx]\n",
        "    \n",
        "    images_in = []\n",
        "    fooling_images = []\n",
        "    deltas = []\n",
        "    \n",
        "    for i, input in enumerate(dataset):\n",
        "      image_in = input[0]\n",
        "      label = input[1]\n",
        "      \n",
        "      fooling_image, delta = make_fooling_image(\n",
        "          image_in,\n",
        "          target_class=target_class,\n",
        "          learning_rate=1e-1,\n",
        "          regularization=5e-5,\n",
        "          num_iter=100,\n",
        "          momentum=0.9)\n",
        "      \n",
        "      delta = 0.5 + (5.0/255.0) * delta\n",
        "    \n",
        "      images_in.append(image_in)\n",
        "      fooling_images.append(fooling_image)\n",
        "      deltas.append(delta)\n",
        "    \n",
        "      if i >= num_images:\n",
        "        break\n",
        "        \n",
        "    print (\"\\nLeft: original, middle: fooling image, right: difference magnified by 5x (gray is 0).\\n\"\n",
        "        \"AlexNet will classify the middle image in each row as %r with high confidence\" % (\n",
        "        labels_id2word[target_class]))\n",
        "    show_image_rows(list(zip(images_in, fooling_images, deltas)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6IMtJDL56bi"
      },
      "source": [
        "## **GradCam**\n",
        "\n",
        "GradCam ~ explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWr8l1Wc9WjV"
      },
      "source": [
        "In "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DM12BKJw54g3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngWh5rqFwRfu"
      },
      "source": [
        "# take input image and cam-mask and make gradCam image\n",
        "def show_cam_on_image(img, mask):\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
        "    heatmap = np.float32(heatmap) / 255\n",
        "    cam = heatmap + np.float32(img)\n",
        "    cam = cam / np.max(cam)\n",
        "\n",
        "    plt.imshow(cv2.cvtColor(np.uint8(255*cam), cv2.COLOR_BGR2RGB))\n",
        "    plt.show()\n",
        "\n",
        "# Preprocess image before entering a pretrained model\n",
        "def preprocess_image(img):\n",
        "    means = [0.485, 0.456, 0.406]\n",
        "    stds = [0.229, 0.224, 0.225]\n",
        "\n",
        "    preprocessed_img = img.copy()[:, :, ::-1]\n",
        "    for i in range(3):\n",
        "        preprocessed_img[:, :, i] = preprocessed_img[:, :, i] - means[i]\n",
        "        preprocessed_img[:, :, i] = preprocessed_img[:, :, i] / stds[i]\n",
        "    preprocessed_img = \\\n",
        "        np.ascontiguousarray(np.transpose(preprocessed_img, (2, 0, 1)))\n",
        "    preprocessed_img = torch.from_numpy(preprocessed_img)\n",
        "    preprocessed_img.unsqueeze_(0)\n",
        "    input = preprocessed_img.requires_grad_(True)\n",
        "    return input\n",
        "\n",
        "# Take the gradient information flowing into the last convolutional layer of the CNN \n",
        "# and features map of last convolutional layer\n",
        "# and then make masks for guided-gradCam \n",
        "def makemask(grads_val, features):\n",
        "  \n",
        "  grads_val, features = gradcam(model, input_path)\n",
        "\n",
        "  target = features\n",
        "  target = target.cpu().data.numpy()[0, :]\n",
        "\n",
        "  weights = np.mean(grads_val, axis=(2, 3))[0, :]\n",
        "  cam = np.zeros(target.shape[1:], dtype=np.float32)\n",
        "\n",
        "  for i, w in enumerate(weights):\n",
        "    cam += w * target[i, :, :]\n",
        "\n",
        "  cam = np.maximum(cam, 0)\n",
        "  cam = cv2.resize(cam, input_img.shape[2:])\n",
        "  cam = cam - np.min(cam)\n",
        "  cam = cam / np.max(cam)\n",
        "\n",
        "  show_cam_on_image(img, cam)\n",
        "\n",
        "  return cam\n",
        "\n",
        "# preprocess guided Backprop and guided grad-Cam to imshow these images\n",
        "def deprocess_image(img):\n",
        "    \"\"\" see https://github.com/jacobgil/keras-grad-cam/blob/master/grad-cam.py#L65 \"\"\"\n",
        "    img = img - np.mean(img)\n",
        "    img = img / (np.std(img) + 1e-5)\n",
        "    img = img * 0.1\n",
        "    img = img + 0.5\n",
        "    img = np.clip(img, 0, 1)\n",
        "    return np.uint8(img*255)\n",
        "\n",
        "# take the model, mask, input image's path to make guided backprop, guided gradCam\n",
        "def Guide_gradcam(model, mask, input_path):\n",
        "    img = cv2.imread(input_path, 1)\n",
        "    img = np.float32(cv2.resize(img, (224, 224))) / 255\n",
        "    input_img = preprocess_image(img)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    changerelu(model)\n",
        "\n",
        "    #Get guided backprop, guided gradCam\n",
        "    output = model(input_img)\n",
        "    index = np.argmax(output.cpu().data.numpy())\n",
        "    one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32)\n",
        "    one_hot[0][index] = 1\n",
        "    one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
        "    one_hot = torch.sum(one_hot * output)\n",
        "\n",
        "    one_hot.backward(retain_graph=True)\n",
        "    output = input_img.grad.cpu().data.numpy()\n",
        "    output = output[0, :, :, :]\n",
        "\n",
        "    gb = output\n",
        "    gb = gb.transpose((1, 2, 0))\n",
        "    cam_mask = cv2.merge([cam, cam, cam])\n",
        "    cam_gb = deprocess_image(cam_mask*gb)\n",
        "    gb = deprocess_image(gb)\n",
        "\n",
        "    plt.imshow(cv2.cvtColor(cam_gb, cv2.COLOR_BGR2RGB))\n",
        "    plt.show()\n",
        "    plt.imshow(cv2.cvtColor(gb, cv2.COLOR_BGR2RGB))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJYhLE6SC4HH"
      },
      "source": [
        "save_grad = []\n",
        "\n",
        "# to get a gradient in intermediate gradient value, it is recommand to use .register-hook function\n",
        "# the hook_func below is used with .register-hook function\n",
        "def hook_func(grad):\n",
        "  save_grad.append(grad)\n",
        "  return grad\n",
        "\n",
        "def gradcam(model, input_path):\n",
        "  img = cv2.imread(input_path, 1)\n",
        "  img = np.float32(cv2.resize(img, (224, 224))) / 255\n",
        "  input_img = preprocess_image(img)\n",
        "  x = input_img\n",
        "\n",
        "  #### PA3 implementation start####\n",
        "  ### You should take the gradient information flowing into the last convolutional layer of the CNN\n",
        "  feature_module = model.layer4\n",
        "  model.eval()\n",
        "  target_activation = \"2\"\n",
        "\n",
        "  for name, module in model._modules.items():\n",
        "    if module == feature_module:\n",
        "      for name2, module2 in feature_module._modules.items():\n",
        "        x = module2(x)\n",
        "        output = []\n",
        "        if name2 in target_activation:\n",
        "          x.register_hook(hook_func)\n",
        "          features = x\n",
        "    elif \"avgpool\" in name.lower():\n",
        "      x = module(x)\n",
        "      x = x.view(x.size(0),-1)\n",
        "    else:\n",
        "      x = module(x)\n",
        "\n",
        "  output = x\n",
        "  \n",
        "  ### placeholder ###\n",
        "  # features : intermediate feature map\n",
        "  # output : model's output\n",
        "\n",
        "  index = np.argmax(output.cpu().data.numpy())\n",
        "  one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32)\n",
        "  one_hot[0][index] = 1\n",
        "  one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
        "  one_hot = torch.sum(one_hot * output)\n",
        "\n",
        "  feature_module.zero_grad()\n",
        "  model.zero_grad()\n",
        "  one_hot.backward(retain_graph=True)\n",
        "\n",
        "  ## grads_val : intermediate feature map's gradient\n",
        "\n",
        "  grads_val = save_grad[0].cpu().data.numpy()\n",
        "\n",
        "  #### PA3 implementation end ####\n",
        "\n",
        "  return grads_val, features\n",
        "\n",
        "\n",
        "# To get a guided Backprop and Guided Grad-CAM, you shuld change the relu function in a pretrained model \n",
        "# with the built-in function according to the GradCam Paper\n",
        "\n",
        "# the code below is to assign built-in function\n",
        "# Source : https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html\n",
        "class MyReLu(Function):\n",
        "    @staticmethod\n",
        "    def forward(self, input):\n",
        "        positive_mask = (input > 0).type_as(input)\n",
        "        output = torch.addcmul(torch.zeros(input.size()).type_as(input), input, positive_mask)\n",
        "        self.save_for_backward(input, output)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(self, grad_output):\n",
        "        input, output = self.saved_tensors\n",
        "        grad_input = None\n",
        "\n",
        "        positive_mask_1 = (input > 0).type_as(grad_output)\n",
        "        positive_mask_2 = (grad_output > 0).type_as(grad_output)\n",
        "        grad_input = torch.addcmul(torch.zeros(input.size()).type_as(input),\n",
        "                                   torch.addcmul(torch.zeros(input.size()).type_as(input), grad_output,\n",
        "                                                 positive_mask_1), positive_mask_2)\n",
        "        return grad_input\n",
        "\n",
        "# the code below is to change relu function with manual built in function\n",
        "def changerelu(model):\n",
        "  for name, module in model._modules.items():\n",
        "    changerelu(module)\n",
        "    if name == 'relu':\n",
        "      model._modules[name] = MyReLu.apply  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFK8vhRdBFEU"
      },
      "source": [
        "model = torchvision.models.resnet50(pretrained=True)\n",
        "input_path = '/gdrive/My Drive/datas/both.png'\n",
        "\n",
        "grads_val, features = gradcam(model, input_path)\n",
        "\n",
        "mask = makemask(grads_val, features)\n",
        "\n",
        "Guide_gradcam(model, mask, input_path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}